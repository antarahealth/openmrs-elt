{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from common.job import Job\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic='encounter-obs-orders' # define topic/s\n",
    "kafka_config = Job.getConfig()['kafka'][topic]\n",
    "ssc= Job.getStreamingContext()\n",
    "kafka_stream = KafkaUtils\\\n",
    "      .createDirectStream(ssc,topics=kafka_config['topics'],kafkaParams=kafka_config['config']) \\\n",
    "      .map(lambda msg: json.loads(msg[1]))\n",
    "\n",
    "obs_stream = kafka_stream \\\n",
    "        .filter(lambda msg: msg['schema']['name'] == 'dbserver1.openmrs.obs.Envelope') \\\n",
    "        .map(lambda msg: msg['payload']['after']) \\\n",
    "        .map(lambda a: Row(**a))\n",
    "\n",
    "orders_stream = kafka_stream \\\n",
    "        .filter(lambda msg: msg['schema']['name'] == 'dbserver1.openmrs.orders.Envelope') \\\n",
    "        .map(lambda msg: msg['payload']['after'])\\\n",
    "        .map(lambda a: Row(**a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Micro-Batch--- \n",
      "Starting calculations for flat_enc_obs_orders Mon Mar 16 21:07:48 2020\n",
      "Took 2.084018 seconds\n"
     ]
    }
   ],
   "source": [
    "from streaming.encounter_job import EncounterJob\n",
    "from pyspark.sql import SparkSession, Row, SQLContext, Window\n",
    "job = EncounterJob()\n",
    "\n",
    "\n",
    "# separate obs with null encounters from obs with encounters\n",
    "obs_with_enc_stream = obs_stream.filter(lambda a: a['encounter_id'] is not None)\n",
    "obs_with_null_enc_stream  = obs_stream.filter(lambda a: a['encounter_id'] is None)\n",
    "\n",
    "#convert orders and obs with encounters into a tuple of encouter_id and person_id\n",
    "orders_stream = orders_stream.map(lambda row: (row['encounter_id'], row['patient_id']))\n",
    "obs_with_enc_stream = obs_with_enc_stream.map(lambda row: (row['encounter_id'], row['person_id']))\n",
    "\n",
    "#union the orders and obs stream\n",
    "enc_obs_orders = obs_with_enc_stream.union(orders_stream)\n",
    "\n",
    "#extract distinct encounter id\n",
    "enc_obs_orders = enc_obs_orders.reduceByKey(lambda x, y: x)\n",
    "\n",
    "#convert into (person_id, encounter_id) tuple from (encounter_id, person_id) tuple\n",
    "enc_obs_orders = enc_obs_orders.map(lambda tpl: (tpl[1], tpl[0]))\n",
    "\n",
    "#convert obs without encounters into person_id, None tuple\n",
    "obs_with_null_enc = obs_with_null_enc_stream.map(lambda row: (row['person_id'], None))\n",
    "\n",
    "#join obs without encounters with the enc_obs_orders for processing\n",
    "enc_obs_orders = enc_obs_orders.union(obs_with_null_enc)\n",
    "\n",
    "#group by patient_id to get distinct patients\n",
    "enc_obs_orders = enc_obs_orders.groupByKey()\\\n",
    "    .map(lambda x : Row(person_id=x[0], encounters=list(filter(None.__ne__, x[1]))))\n",
    "\n",
    "enc_obs_orders.foreachRDD(lambda rdd: job.run_microbatch(rdd))\n",
    "\n",
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('anaconda3': virtualenv)",
   "language": "python",
   "name": "python37464bitanaconda3virtualenvefdffc192b144e2b98dfc27892fa181f"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
